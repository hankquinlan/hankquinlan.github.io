---
layout: post
title:  "Data Science Festival Live 2021"
date:   2021-11-27 09:00:00 +0000
categories: Data Science
permalink: /:title
---

Introduction
============
It was great to get back to an IRL conference after nearly two years. The Data Science Festival Live 2021 took place at Code Node, South Place just off Moorgate in London on 27th November 2021. It was equally wonderful to be back at Skills Matter for the first since its resurrection.

In what follows, the descriptions are from the talks I personally went to, following my particular interests.

The full agenda is attached at the bottom of this post.

Robots Everywhere - The Role of Data Science and Automation in the Creative Industry by Magda Woods, Waive
==========================================================================================================

In this talk Magda discusses how automation has removed the need for repetitive, routine and boring tasks to be done manually, for example the collection of news feeds, a task that that used to be a cut and paste one. In the creative industry the focus must be on deploying data and data science skills to tasks none of your colleagues want to do. The presentation goes into details on what was necessary to automate or assist people in doing their jobs. It also looks at the reasons why technological progress with language models, like GPT-3 have not yet scaled into the newsrooms.

How do you extract value from data?
- Sell it
- Add value for the customer
- Reduce operational cost

An interesting reflection was on the BBC recommender system and the question of whether its recommendations were too narrow based on what viewers were watching. Often viewers just view a small set of programmes. Maybe recommender systems should try to broaden horizons rather just serve up the same all thing.

But of course there is a tension here. Sites want to keep you there by recommending things that you will want see. However, that is probably not good for you.

Magda referred to a Harvard Business Review Technology And Analytics article "The Next Big Breakthrough in AI Will Be Around Language" (https://hbr.org/2020/09/the-next-big-breakthrough-in-ai-will-be-around-language). I thought these breakthroughs were already being made myself, and have been for decades. People leading organizations jump on such headlines and decide this is the way to go. However, what is not  clear is what this "Next Big Breakthrough" will be given that breakthroughs are happening all the time anyway. Many big impact changes come fairly simple innovations.

The various GPT versions need structured exhaustive data to work but what is typically available is messy unstructured data with gaps and inconsitencies.

Automation allows content creators to concentrate on what really matters to them and frees up resources that can be better deployed.

Low cost, low impact endeavours can be automated.

Motivated by what is seen in Google trends browser, some scipts have been written to automatically pull data from the curated sources and move it from place to place, meaning a repetitive task can be automated.

Carmen is a script for identifying data points in data heavy articles.

Such things are not blockbusters but small incremental changes that actually make a difference.

Data Science leaders should concentrate on automation opportunities where:
- Scale is required
- A requirement to reduce risk in decision making
- Human work is involved in a low impact way, an example being Carmen

A question was asked about recommender systems. The real task for data sceientists is to fill in the gaps in the data that recommender systems miss.

Should content creators have control over their material or be driven by what recommender systems will recommend? In the end content creators do notwant to produce something no one wants to read.

An interesting question about trends in translation and journalistic tones of voice. Huge scope for translationn given the World Service's provision across may languages.

Another question was about recommender systems being driven by revenue. Is there a way to look at them diferently?
There are, such as people being motivated to return to the site and being served up more diverse content. This however requires a change in mindset so the motivation is not always the bottom line driven by the number of clicks.

Of course there are many sites that are not driven by revenue. And of course the BBC is a great example of this. There is such a thing as the BBC Public Service Algorithm (https://www.bbc.co.uk/news/entertainment-arts-48252226), the hope of which is that "audiences will stumble onto something new, instead of content that simply reinforces their views".

This was a very interesting talk that covered a lot of ground and was very thought provoking

Data Quality with LV=’s Input–Checker by Ned Webster, LV=
=========================================================

Ned talked about Input Checker, a python package for checking if data stored in pandas data frames meets a number of prerequisites. 

Input Checker is one of three projects that LV has opened sourced at https://github.com/lvgig, the others being tubular, a python package using transformers for pre-processing, and test-aide, package to support unit testing in data science projects.

Ned explored what we mean by data quality. He described some existing frameworks for checking data quality, notably Great Expectations,  a shared, open standard for data quality (https://greatexpectations.io/). He also described an enabler for Input Checker, Marshmallow, an ORM/ODM/framework-agnostic library for converting complex datatypes, such as objects, to and from native Python datatypes.

Input Checker enables users to compare a given data frame against a benchmark data frame. This is similar to the Golden Master concept, which is a saved copy of the output a program generates.


Input Checker checks the following to ensure the mutated data frame is the same as the original in the following respects:

- Nulls
- Dtypes
- Categorical values
- Numerical values
- Datetime values

This is a very useful tool when reorganising the code.

The realities of building domain-specific language models for production by Matt Harding & Stanimir Vichev, London Stock Exchange Group (LSEG)
==============================================================================================================================================

LSEG Labs have built a set of domain-specific language models, based on Google’s BERT architecture, using LSEG’s proprietary financial data. 

The talk describes taking these models from inception to production describing all the problems encountered on the way, from testing the waters to convicing decision makers that the project was worth investing in. NLP was fundamental in this endeavour.

LSEG uses a Financial News Language model, derived through the pre-processing of financial news from the Reuters News Archive (RNA). BERT-RNA as it is known, is pre-trained on top BERT base of using the RNA. The archive contains all Reuters articles published between 1996 and 2019.

Training took place on GCP Preemptible TPUs and running inference via AWS Batch Transform. The tech stack is made up of Python, SpaCy, Regex, S3 and Athena,  which works directly with data stored in S3 using standard SQL.

Matt and Stan then discussed how the model was benchmarked using a downstream classification task. 

They went onto discuss their plans for future, including experimenting with different BERT architectures, introducing MLOps, using a feauture store for inference training, and using other financial news datasets.

In conclusion they discussed the pros and cons of different ways of providing the models to their customers.

A/B testing: Setting up for success & choosing the right Statistical Framework by Benjamin Pettit, Cleo
=======================================================================================================

Cleo is a chatbot for financial health.  In this talk Benjamin goes through has been learned from running a multitude of A/B tests at Cleo.
A/B testing is being used more, but there confilicting aims in using this approach.

The aim is to to measure which versions are better and improve the product by exploring the best bets. A "Holdout" is a long running A/B test, the aim of which is to help track long term impacts on Key Performance Indicators (KPIs) and the long tem benefits of multiple tests. It works by not exposing a small propertion of the users to any changes.

Binary  vs  continous stats tests. Talk is abou binary

Another choice is between Bayesian vs Frequentist frameworks.

For Frequentist a minimal sample size needs to chosen so an effect can be measured. For Bayesian maximum expected loss might be chosen.

Cleo prefers Bayesian statistics as something like win rate is more easily explained that a p-value. What is the chance that product A is better than B? Also the expected loss reveals the magnitude of the risk. This also determines when any test is stopped.

The speaker went on to give and overview of Bayesian Stats and where the win rate comes from.

Their appraoch uses an uninformed prior belief about the success rate. The stronger the informed prior, the longer it will take for the observed data to be affected by the test.

He continued to describe the decisions around when to stop the test.

The experiment run cycle can be described as follows:

- Collect Data: Control vs Treatment(s)
- Calculate win rate
- Apply Decison Rule
-- Option 1: ROPE (Region of Practical Equivalence)
-- Option 2: Expected Loss
- Experiment Conclusion based on conclusion or exceeded allowed run time

Decision Rules:
- Expected Loss means if the current winner is worse how would be lost and is this above the threshold of caring?
- ROPE: Are the most likely effect sizes far fom zero or within the Region of Practical Equivalence?

Concluding remarks examined the case for Bayesian versus Frequentist Stats, which involves aking questions such as:

- A success metric and stats test beforehand?
- A decision rule matching risk appetite?

Bayesian Stats allows fast iteration while quantifying risk.

Question: cross validation instead of holdout? The answer seemed to be no, as a holdout is trying to measure the impact of particular squad. It is hard to see how cross validation would be practical here.

Using hybrid recommender system to personalise Financial Times push notifications by Adam Gajtkowski, Finacial Times
====================================================================================================================

Below is an overview of the model of the FT's recommender system:

Article Vectorization -> Unsupervised Clustering -> Bayesian Shrinkage -> Breadth of Reading Metric
                                                  |
                                                  V
                                          User Reading History
                                                  |
                                                  V
                                           Machine Learning
                                                  |
                                                  V
                                           Recommendations
                                            
Each of these components is discussed below. 

Article Vectorization is based around Doc2Vec for document embeddings. The Paragraph Vector version of the model is used to transform the vectorisation problem into a classification problem. This samples a word from a sentence treating the sampled word as a predicted variable, and nearby sampled words as predictors.

Initial clustering based on the vectorised article is done using top-down hierarchical clustering which was judged to be the best performing.

The next is reclustering to detect new trends and update established clusters. This involves discovering articles that are outliers to existing clusters
and then creating new clusters from outliers, determining the number of clusters based on the height cut-off point of the dendrogram.  This is a tree-structured graph often used in heat maps to visualize the result of a hierarchical clustering calculation.

Bayesian shrinkage tries to account for the evidence of the preferences of a real using user for a particular group of articles.

The Breadth of Reading Metric looks at which articles should actually recommended to users, based on if their interests are narrowly or broadly based, i.e. their reading habits.

The main purpose of this project was to reduce churn by increasing the breadth of reading amongst subscribers, in which aim it seems to have succeeded.

Connecting the Dots: Harnessing the Power of Graphs & ML by Ebru Cucen, Opencredo 
=================================================================================

I didn't take many notes on this talk but it was nonetheless interesting. It attempted to provide an overview of how graphs can help improve upon the predictions and answers arrived at by using traditional ML algorithms and approaches alone. 

It goes over combining the power of graphs and Machine Learning, ending up with better answers compared to using standard ML approaches alone. 

It includes diving into some demos of key approaches to illustrate and explain the ideas along the way.

More specifically the talked about Knowledge Graphs. When we construct one we end up with a Question and Answer resource. Google implemented a knowledge graph in 2012 which turned their search engine into a Q&A engine.

The speaker also talked about graph algorithms, and more specifically the path finding algorithm and its uses.

"What is the shortest path from Victoria to Liverpool Street?" you may well ask Google.

The result that was presented was far from the shortest path, in fact it seemed to be a choice of longest paths. The shortest path was not on the graph (via Oxford Circus). This was atributed to the dataset used. Hmm, not a particularly successful demo.

The speaker then talked about Random Walks, which can be likened to a drunk man walking. The algorithm starts at one node and randomly follows one of the relationships forward or backward to a neighbour node, then does the same from that node and so on. This is helpful in sampling.

We then what onto neual nets and their properties. Then to calculating the loss function. 

Karate Club with GCN - Greedy Clustering. mention torch.geometric.GCNConv as an implementaion of this idea.

Coming on to recommendations, Pinsage from Pinterest was described as a latest SOTA recommendation engine.

The talk concluded by describing the growing trend in publications in graphs, graph networks and graph neural neworks.
 
A free book on GraphML by Ebru Cucen is available from the Opencredo website.

Fighting bad information with AI by Alex Joseph Full Fact
=========================================================

The founder of Full Fact, Will Moy gave at the DSF a few years, so it was good to hear about developments at Full Fact since then.

Began with a definition of what bad information is, e.g. Covid and 5G.

Full Fact has been fighting disinformation since 2010. It is primarily aimed augmented the knowledge of fact checkers.

Full Fact Alpha is now their main product. It relies on lots of news articles and what is described as "useful" AI. I guess there is a lot of useless AI out there.

100,000 senetnces are collected each day, but it appears that only 30% of them can be fact checked. Why is this? A lot turn out to be opinions or predictions, i.e. not quantative assertions. This is what the Claim Detection Tool does.

This tool has 50,000 sentences, 10 labels and is uses Multiligual BERT as its language model and is laready have an impact in production.

Some claims have already been fact checked, so it's important that these claims can be matched. For any two claims a and b, recognizing one is true leads to the conclusion b is also true, and vice versa. This is known as bidirectional entailment, which implies more than similar tasks such as semantic similarity and paraphrase detection. The challenges faced revolve around ambiguity and context.

Annotating claims represents a particular challenge.

Full Fact set up a Claim Annotation Challenge in 2021 which can be found on their web site. This contributed a great deal to creating a data set of the size that was needed for this task.

Full Fact take a ensemble model approach, involving sentence-BERT embeddings, BM25, an IR algorithm and Named Entity Comparison

sentence-BERT is a specialized BERT model aimed at semantic similarity. BM25 can be thought of as a variation of td-idf to search for overlapping sentences. Named Entity Comparison uses NER, recognizing that similar claims use the same named entities.


Included in Full Fcat Alpha is a Checked Claim Digest Tool, bringing together claims that have been fact checked. It's not perfect, but it's often able to identify potential concrete matches. Human fact checkers can you use the tool to confirm that these are indeed correct matches. 

One interesting question was how to prevent malicious annotations using the annotation tool. One way of guarding against is to show the annotation to a number of other annotators so it can be cross validated.

Multiligual BERT is good at revealing the structure and meaning of the sentences so this is pivotal in Full Fact's approach.

It turns out that you can have claims fact checked by forwarding them to a Full Fact WhatsApp number.

They don't try identify false spreaders, but concentrate on the claims being made.

BERT it turns out has a hard time comparing numbers, e.g. the 350 million nhs claim against any other amount. So matches can be made on entirely different numbers, but may be with the same entities.

Can BERT be fine tuned to recognise the same numbers in claims? Maybe.

Checkiardo, Argentina and Africa Check

Full fact won't release their dataset for fear of malicious actors gaining insight into how they work.

How effective is this kind of fact checking? Reliant on academic research, but their own research suggests it is effective. But what is the goal?

Misinformative claims that are most likely to spread seem to be the most controversial ones.

I very much enjoyed this talk, gaining a insight into Data Science and NLP can be used for good.

Vigorous communication as the key ingredient of a successful data science career by Roberto Medri, Meta (Instagram)
===================================================================================================================

Roberto Medri was speaking live in London, but not at CodeNode. Sadly I took no notes.

In conclusion, this was a greatly enjoyable day. The descriptions above are what I heard and any mistakes or misinterpretations are down to me.
Luckily the whole thing is available on youtube at https://www.youtube.com/playlist?list=PLwzX3WM0AfxrRuDzpH6jNXUCF1n303t45.

Here is the schedule:
[DSF_Schedule_A4Sheet_v8.pdf](https://github.com/mbateman/mbateman.github.io/files/8179259/DSF_Schedule_A4Sheet_v8.pdf)
